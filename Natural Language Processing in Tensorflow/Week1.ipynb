{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week1.ipynb","provenance":[],"authorship_tag":"ABX9TyNa0XBBmwvUqEjTU74fdjxB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"j4riYdNFaiBb"},"source":["The machine learning algorithms need to work with number so if we want to work with sentences and words we need to convert each word to a number like below:\n","\n","I love My dog -> {1:'i', 2:'love', 3:'my', 4:'dog'} -> [[1, 2, 3, 4]]\n","\n","now we could pass it throug our neural network to work with this data \n","for creating this list we can use a built-in api of tensorflow called Tokenizer."]},{"cell_type":"code","metadata":{"id":"Ne7Pu8FsQbxj"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.text import Tokenizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_SHOypgAcGyS"},"source":["tokenizer = Tokenizer(num_words=100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I0VJcyH9cUI8","executionInfo":{"status":"ok","timestamp":1622278049788,"user_tz":-270,"elapsed":363,"user":{"displayName":"soroush ziaee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgK1aDor2OVKZCniE6RZU0Z7cuWTAm_dgiD-rJe4g=s64","userId":"14982629694297141642"}},"outputId":"1ab2d618-9958-468f-aa97-0679087577e6"},"source":["sentences = [\n","             'I love milad soleymani',\n","             'I love myself',\n","             'I hate to be laghar',\n","             'I want to be Topol'\n","]\n","\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","print(word_index)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'i': 1, 'love': 2, 'to': 3, 'be': 4, 'milad': 5, 'soleymani': 6, 'myself': 7, 'hate': 8, 'laghar': 9, 'want': 10, 'topol': 11}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Oan6gjpcdMwV"},"source":["After Doing the above staff now we need to create a sequence of sentence into the list with code below:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VuVIA2Lic8kO","executionInfo":{"status":"ok","timestamp":1622278053337,"user_tz":-270,"elapsed":359,"user":{"displayName":"soroush ziaee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgK1aDor2OVKZCniE6RZU0Z7cuWTAm_dgiD-rJe4g=s64","userId":"14982629694297141642"}},"outputId":"f0174bc3-f508-463f-ee67-028755fd761f"},"source":["sequences = tokenizer.texts_to_sequences(sentences)\n","print(sequences)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[1, 2, 5, 6], [1, 2, 7], [1, 8, 3, 4, 9], [1, 10, 3, 4, 11]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9DiuLzu2TXb4","executionInfo":{"status":"ok","timestamp":1622278078495,"user_tz":-270,"elapsed":1235,"user":{"displayName":"soroush ziaee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgK1aDor2OVKZCniE6RZU0Z7cuWTAm_dgiD-rJe4g=s64","userId":"14982629694297141642"}},"outputId":"a0a8d16d-2729-4d4b-e483-097343a14a3c"},"source":["temp = ['milad ashegh laghar']\n","print(tokenizer.texts_to_sequences(temp))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[5, 9]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3FnKkaDxduSF"},"source":["As you can see here there are 4 sentenses with different shapes, that could be a problem in further process so we need to use padding in this cases."]},{"cell_type":"code","metadata":{"id":"mSCym-gyeZ9k"},"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MK7HebLQehPI","executionInfo":{"status":"ok","timestamp":1621928757353,"user_tz":-270,"elapsed":289,"user":{"displayName":"soroush ziaee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgK1aDor2OVKZCniE6RZU0Z7cuWTAm_dgiD-rJe4g=s64","userId":"14982629694297141642"}},"outputId":"65e366a5-a96d-4102-9ff2-c017367ac00a"},"source":["# maxlen -> number of pad to add end of the sentences\n","# padding -> do padding from the pre or post of sentences\n","# truncating -> depend on maxlen say where to remove the sentence from\n","\n","padded = pad_sequences(\n","    sequences,\n","    maxlen = 10,\n","    padding='post',\n","    truncating='post'\n",")\n","\n","padded"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 2,  3,  6,  7,  0,  0,  0,  0,  0,  0],\n","       [ 2,  3,  8,  0,  0,  0,  0,  0,  0,  0],\n","       [ 2,  9,  4,  5, 10,  0,  0,  0,  0,  0],\n","       [ 2, 11,  4,  5, 12,  0,  0,  0,  0,  0]], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"yhHnD6jTfS8S"},"source":["Now we want to work with real Data of BBC to see if a headline is sarcasm or not"]},{"cell_type":"markdown","metadata":{"id":"m5WDTssWfq5H"},"source":["First we should Download the dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hfRAODwwft11","executionInfo":{"status":"ok","timestamp":1621928906355,"user_tz":-270,"elapsed":12,"user":{"displayName":"soroush ziaee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgK1aDor2OVKZCniE6RZU0Z7cuWTAm_dgiD-rJe4g=s64","userId":"14982629694297141642"}},"outputId":"f8068a61-3736-41cc-e132-fa5618fea292"},"source":["!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json \\\n","    -O /tmp/sarcasm.json"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-05-25 07:48:26--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sarcasm.json\n","Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.13.80, 172.217.13.240, 172.217.15.80, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.13.80|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5643545 (5.4M) [application/json]\n","Saving to: ‘/tmp/sarcasm.json’\n","\n","\r/tmp/sarcasm.json     0%[                    ]       0  --.-KB/s               \r/tmp/sarcasm.json   100%[===================>]   5.38M  --.-KB/s    in 0.02s   \n","\n","2021-05-25 07:48:26 (261 MB/s) - ‘/tmp/sarcasm.json’ saved [5643545/5643545]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j7ztaLNofwsq"},"source":["import json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b5olikxFf0Pa"},"source":["with open('/tmp/sarcasm.json', 'r') as fin:\n","    dataset = json.load(fin)\n","\n","sentences = []\n","labels = []\n","urls = []\n","\n","\n","for item in dataset:\n","    sentences.append(item['headline'])\n","    labels.append(item['is_sarcastic'])\n","    urls.append(item['article_link'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r-RefmV4gkqq"},"source":["Next Step is tokenize the loaded data..."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eNQ-gCGZgiOO","executionInfo":{"status":"ok","timestamp":1621929220763,"user_tz":-270,"elapsed":660,"user":{"displayName":"soroush ziaee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgK1aDor2OVKZCniE6RZU0Z7cuWTAm_dgiD-rJe4g=s64","userId":"14982629694297141642"}},"outputId":"5ae09085-6b09-4f98-a057-f8bf150c4cc1"},"source":["tokenizer = Tokenizer(oov_token='<OOV>')\n","tokenizer.fit_on_texts(sentences)\n","\n","word_index = tokenizer.word_index\n","print('Data Shape is :', len(word_index))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Data Shape is : 29657\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0zet8jDTg9-P"},"source":["Now we need to create sequences in order to prepare data to pass through our network..."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SSbqYp9Zg9UR","executionInfo":{"status":"ok","timestamp":1621929388505,"user_tz":-270,"elapsed":1235,"user":{"displayName":"soroush ziaee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgK1aDor2OVKZCniE6RZU0Z7cuWTAm_dgiD-rJe4g=s64","userId":"14982629694297141642"}},"outputId":"f6af09c4-5704-419b-eddb-3855a12696d8"},"source":["sequences = tokenizer.texts_to_sequences(sentences)\n","padded_sequences = pad_sequences(sequences, padding='post')\n","\n","print(sentences[2])\n","print(padded_sequences[2])\n","\n","print(padded_sequences.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["mom starting to fear son's web series closest thing she will have to grandchild\n","[  145   838     2   907  1749  2093   582  4719   221   143    39    46\n","     2 10736     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0]\n","(26709, 40)\n"],"name":"stdout"}]}]}